from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from dotenv import load_dotenv
import os

# åŠ è½½çŽ¯å¢ƒå˜é‡
load_dotenv()

# ä»Ž .env æ–‡ä»¶ä¸­èŽ·å– HuggingFace Token
HF_API_TOKEN = os.getenv("URL_TOKEN")

# ç¡®ä¿ Token å­˜åœ¨
if HF_API_TOKEN is None:
    raise ValueError("HuggingFace API Token is missing!")

print("ðŸ§  æ­£åœ¨åŠ è½½ ZSYAI æ¨¡åž‹ï¼ˆMistral 7Bï¼‰...")

# åŠ è½½ Mistral 7B æ¨¡åž‹å’Œ tokenizer
model_id = "mistralai/Mistral-7B-Instruct-v0.3"  # ä½¿ç”¨æ­£ç¡®çš„æ¨¡åž‹ ID
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_API_TOKEN)
model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=HF_API_TOKEN)

model.eval()  # åªç”¨æŽ¨ç†æ¨¡å¼
print("âœ… ZSYAI æ¨¡åž‹åŠ è½½å®Œæˆ")

def zsy_reply(messages: list[dict]) -> str:
    history = []
    for msg in messages:
        if msg["role"] == "user":
            query = msg["content"]
            response, history = model.chat(tokenizer, query, history=history)
        elif msg["role"] == "assistant":
            history.append((msg["content"], ""))
    return response

