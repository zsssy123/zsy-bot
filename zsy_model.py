# zsy_model.py
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print("ğŸ§  æ­£åœ¨åŠ è½½ ZSYAI æ¨¡å‹ï¼ˆMistral 7Bï¼‰...")

HF_API_TOKEN =os.getenv("URL_TOKEN")
# ä½¿ç”¨é‡åŒ–åçš„ Mistral-7Bï¼ˆint4 ç‰ˆæœ¬ï¼‰
model = AutoModelForCausalLM.from_pretrained("mistralai/mistral-7b-instruct", revision="int4", device_map="auto",use_auth_token=HF_API_TOKEN)
tokenizer = AutoTokenizer.from_pretrained("mistralai/mistral-7b-instruct", use_auth_token=HF_API_TOKEN)

model.eval()  # åªç”¨æ¨ç†æ¨¡å¼
print("âœ… ZSYAI æ¨¡å‹åŠ è½½å®Œæˆ")

def zsy_reply(messages: list[dict]) -> str:
    history = []
    for msg in messages:
        if msg["role"] == "user":
            query = msg["content"]
            response, history = model.chat(tokenizer, query, history=history)
        elif msg["role"] == "assistant":
            # ä¿ç•™å†å²ä¸Šä¸‹æ–‡
            history.append((msg["content"], ""))
    return response
