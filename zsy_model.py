# zsy_model.py

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from dotenv import load_dotenv
import os

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# ä» .env æ–‡ä»¶ä¸­è·å– HuggingFace Token
HF_API_TOKEN = os.getenv("URL_TOKEN")

# ç¡®ä¿ Token å­˜åœ¨
if HF_API_TOKEN is None:
    raise ValueError("HuggingFace API Token is missing!")

# æ‰“å°åŠ è½½çŠ¶æ€
print("ğŸ§  æ­£åœ¨åŠ è½½ ZSYAI æ¨¡å‹ï¼ˆMistral 7B æˆ–å…¶ä»–æ¨¡å‹ï¼‰...")

# é€‰æ‹©æ›´å°çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ LLaMA-7Bï¼‰æˆ–è€… Mistral 7Bï¼Œé¿å…ä½¿ç”¨è¿‡å¤§çš„æ¨¡å‹
model_id = "mistralai/Mistral-7B-Instruct-v0.3"  # ä½¿ç”¨å…¬å¼€çš„ Mistral-7B æˆ– LLaMA 7B

# åŠ è½½ tokenizer å’Œ modelï¼Œç¡®ä¿ä½¿ç”¨ HuggingFace Token
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=HF_API_TOKEN)
model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=HF_API_TOKEN, revision="int8", device_map={"cpu": 0})

# åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼
model.eval()

print("âœ… ZSYAI æ¨¡å‹åŠ è½½å®Œæˆ")

# è®¾ç½®æœ€å¤§è¾“å…¥ token æ•°é‡æ¥æ§åˆ¶å†…å­˜ä½¿ç”¨
MAX_INPUT_TOKENS = 256  # æ¯æ¬¡æ¨ç†è¾“å…¥æœ€å¤š 256 ä¸ª token

def zsy_reply(messages: list[dict]) -> str:
    """
    å¤„ç†ç”¨æˆ·çš„å¯¹è¯æ¶ˆæ¯ï¼Œå¹¶ç”Ÿæˆ ZSYAI æ¨¡å‹çš„å›å¤ã€‚
    
    Args:
        messages: ç”¨æˆ·å’Œ AI ä¹‹é—´çš„å¯¹è¯å†å²åˆ—è¡¨ã€‚
    
    Returns:
        å›å¤å­—ç¬¦ä¸²
    """
    history = []

    # å¤„ç†æ¯æ¡æ¶ˆæ¯
    for msg in messages:
        if msg["role"] == "user":
            query = msg["content"][:MAX_INPUT_TOKENS]  # æˆªæ–­è¾“å…¥åˆ°æœ€å¤§ token æ•°é‡
            # è·å–æ¨¡å‹çš„å›å¤å¹¶ä¿ç•™å†å²è®°å½•
            response, history = model.chat(tokenizer, query, history=history)
        elif msg["role"] == "assistant":
            # å¦‚æœæ˜¯ AI å›å¤ï¼ŒåŠ å…¥å†å²è®°å½•
            history.append((msg["content"], ""))

    # è¿”å›æ¨¡å‹çš„æœ€ç»ˆå›å¤
    return response
